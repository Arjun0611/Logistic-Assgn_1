{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6eb969-976c-45e5-93d8-cdcd3aa60aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92323844-f17f-4c66-bace-134cd0d7af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "# Predicts continuous numeric values.\n",
    "# Equation is y = mx + c\n",
    "# Output range is unbounded.\n",
    "\n",
    "# Logistic Regression:\n",
    "\n",
    "# Nature: Predicts probabilities for binary classification.\n",
    "# Equation:1 / (1 + exp(-mx + b))\n",
    "# Output Range: Constrained between 0 and 1.\n",
    "\n",
    "# Example Scenario for Logistic Regression:\n",
    "# Scenario: Predicting whether an email is spam or not.\n",
    "# Reasoning: Outputs probabilities, making it suitable for binary classification tasks where class probabilities matter.\n",
    "\n",
    "# In this scenario, logistic regression is more appropriate because it provides a probability output, making it suitable for binary classification problems like spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7885df4b-41d0-40bd-a0b6-e7aa8c047aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "\n",
    "# Cost Function in Logistic Regression:\n",
    "\n",
    "# Log Loss function - Represents the difference between predicted probabilities and actual labels.\n",
    "\n",
    "# Optimization:\n",
    "\n",
    "# Method:\n",
    "# Gradient Descent or variants like Stochastic Gradient Descent.\n",
    "# Objective: Minimize the cost function by iteratively adjusting weights (theta) and biases.\n",
    "# Update Rule: Adjust parameters in the direction that reduces the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0893eee-eb11-417a-ba26-34de353b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# Regularization in Logistic Regression:\n",
    "# Technique to prevent overfitting by adding a penalty term to the cost function.\n",
    "\n",
    "# Objective: Discourage complex models by penalizing large coefficient values.\n",
    "\n",
    "# Types: L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "# Impact: Penalizes large coefficients, leading to a simpler model.\n",
    "# Helps prevent overfitting by discouraging overly complex decision boundaries.\n",
    "\n",
    "# In logistic regression, regularization adds a penalty term to the cost function, discouraging complex models and mitigating overfitting. Regularization strength is controlled by the parameter lambda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4aa6634-003d-4ea2-90af-c650d2e0f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# ROC Curve (Receiver Operating Characteristic):\n",
    "# Graphical representation of the trade-off between true positive rate (sensitivity) and false positive rate.\n",
    "# Usage in Logistic Regression: Evaluates the performance of a binary classification model, like logistic regression.\n",
    "# Plotting: Plots true positive rate (y-axis) against false positive rate (x-axis) at various probability thresholds.\n",
    "# Diagonal Line: Represents random chance; a good model should curve above this line.\n",
    "# AUC-ROC (Area Under the ROC Curve): Quantitative measure; higher AUC-ROC indicates better model performance.\n",
    "# Interpretation: Higher true positive rates at lower false positive rates imply better discrimination.\n",
    "# The ROC curve and AUC-ROC provide a comprehensive evaluation of a logistic regression model's ability to discriminate between classes across different probability thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167a76f5-532e-486a-aa5a-29ca595aceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nCommon Techniques:\\n\\nUnivariate Feature Selection: Selects features based on statistical tests.\\nRecursive Feature Elimination (RFE): Iteratively removes less significant features.\\nL1 Regularization (Lasso): Penalizes less important features, encouraging sparsity.\\nInformation Gain: Measures reduction in entropy after adding a feature.\\nForward and Backward Selection: Iteratively adds or removes features based on model performance.\\n\\nPerformance Improvement:\\n1. Model Simplicity:\\nReduces complexity, preventing overfitting.\\n2. Enhanced Interpretability:\\nFocuses on the most relevant features for better understanding.\\n3. Computational Efficiency:\\nReduces computational load by using a subset of features.\\nFeature selection in logistic regression improves performance by emphasizing relevant features, enhancing model simplicity, interpretability, and computational efficiency, while avoiding overfitting.\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Common Techniques:\n",
    "\n",
    "Univariate Feature Selection: Selects features based on statistical tests.\n",
    "Recursive Feature Elimination (RFE): Iteratively removes less significant features.\n",
    "L1 Regularization (Lasso): Penalizes less important features, encouraging sparsity.\n",
    "Information Gain: Measures reduction in entropy after adding a feature.\n",
    "Forward and Backward Selection: Iteratively adds or removes features based on model performance.\n",
    "\n",
    "Performance Improvement:\n",
    "1. Model Simplicity:\n",
    "Reduces complexity, preventing overfitting.\n",
    "2. Enhanced Interpretability:\n",
    "Focuses on the most relevant features for better understanding.\n",
    "3. Computational Efficiency:\n",
    "Reduces computational load by using a subset of features.\n",
    "Feature selection in logistic regression improves performance by emphasizing relevant features, enhancing model simplicity, interpretability, and computational efficiency, while avoiding overfitting.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca8e53b-bf65-4e4f-bf79-a845e30e9023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommon Strategies:\\n\\n1. Resampling: Oversampling minority class or undersampling majority class.\\n2. Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique).\\n3. Weighted Classes: Assign higher weights to minority class in the logistic regression model.\\n4. Anomaly Detection Techniques: Identify minority class instances as anomalies.\\n5. Ensemble Methods: Use ensemble algorithms designed for imbalanced data, like Balanced Random Forest.\\n\\nRationale:\\n\\nMitigates bias towards majority class, improves minority class prediction.\\nPrevents the model from being dominated by the majority class.\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6.\n",
    "\n",
    "\"\"\"\n",
    "Common Strategies:\n",
    "\n",
    "1. Resampling: Oversampling minority class or undersampling majority class.\n",
    "2. Synthetic Data Generation: Techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "3. Weighted Classes: Assign higher weights to minority class in the logistic regression model.\n",
    "4. Anomaly Detection Techniques: Identify minority class instances as anomalies.\n",
    "5. Ensemble Methods: Use ensemble algorithms designed for imbalanced data, like Balanced Random Forest.\n",
    "\n",
    "Rationale:\n",
    "\n",
    "Mitigates bias towards majority class, improves minority class prediction.\n",
    "Prevents the model from being dominated by the majority class.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f3e5d1-a612-4c66-beb6-bbf6434d3ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommon Issues:\\nMulticollinearity:\\nSolution: Use techniques like variance inflation factor (VIF) or feature selection to address multicollinearity.\\nOverfitting:\\nSolution: Regularize the model using L1 or L2 regularization, or use feature selection.\\nImbalanced Datasets:\\nSolution: Employ techniques like resampling, synthetic data generation, or weighted classes.\\nOutliers:\\nSolution: Identify and handle outliers through robust statistical methods or transformation.\\nNon-Linearity:\\nSolution: Transform features or use polynomial features to capture non-linear relationships.\\n\\nBenefits of Addressing Issues:\\n1. Improved Generalization:\\nMitigates overfitting, leading to better model generalization.\\n2. Robustness:\\nHandling multicollinearity and outliers increases model robustness.\\n3. Balanced Predictions:\\nAddressing imbalanced datasets results in more balanced predictions.\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7.\n",
    "\n",
    "\"\"\"\n",
    "Common Issues:\n",
    "Multicollinearity:\n",
    "Solution: Use techniques like variance inflation factor (VIF) or feature selection to address multicollinearity.\n",
    "Overfitting:\n",
    "Solution: Regularize the model using L1 or L2 regularization, or use feature selection.\n",
    "Imbalanced Datasets:\n",
    "Solution: Employ techniques like resampling, synthetic data generation, or weighted classes.\n",
    "Outliers:\n",
    "Solution: Identify and handle outliers through robust statistical methods or transformation.\n",
    "Non-Linearity:\n",
    "Solution: Transform features or use polynomial features to capture non-linear relationships.\n",
    "\n",
    "Benefits of Addressing Issues:\n",
    "1. Improved Generalization:\n",
    "Mitigates overfitting, leading to better model generalization.\n",
    "2. Robustness:\n",
    "Handling multicollinearity and outliers increases model robustness.\n",
    "3. Balanced Predictions:\n",
    "Addressing imbalanced datasets results in more balanced predictions.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cacf5a5-d6a6-4e8d-9a09-de4a3057a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143d83f8-f64c-4c01-94ae-a5b893c11e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ff38e-73e4-47c3-8aeb-fda6ae479a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
